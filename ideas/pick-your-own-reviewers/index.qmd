---
title: "Pick your own reviewers"
author: "Experiments in Assessment WG"
date: "2025-03-03"
categories: [Level 0, CHALLENGE Process Culture, CoARA Commitment 1, CoARA Commitment 2, USER Funders, USER Institutes, USER Academies, USER Research Groups, USER Scientific editors and publishers] 
description: "Applicants recommend peer reviewers for their projects, who are then included in the evaluation process. This approach reduces the burden on evaluation bodies and potentially allows individuals with deep subject knowledge to participate. It can also bring in users of research and non-traditional peer reviewers, helping to diversify the reviewer pool."
image: false
---

<!--
GENERAL GUIDELINES:
- Use this guide to style all text: https://quarto.org/docs/authoring/markdown-basics.html
- Tags (apply as relevant):
Level of completeness: 0
Level of completeness: 1
Level of completeness: 2
Level of completeness: 3
Level of completeness: 4
Level of completeness: 5
Challenge: create an assessment process based upon what the different players in research value
Challenge: make research more inclusive and equitable through assessment
Challenge: mitigate the negative effects of bias on assessment
Challenge: recognize process and culture through assessment
Challenge: foster diversity through assessment
Challenge: foster collaboration through assessment
Challenge: ask assessment questions differently
Commitment: 1. Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
Commitment: 2. Base research assessment primarily on qualitative evaluation for which peer review is central, supported by responsible use of quantitative indicators
Commitment: 3. Abandon inappropriate uses in research assessment of journal- and publication-based metrics, in particular inappropriate uses of Journal Impact Factor (JIF) and h-index
Commitment: 4. Avoid the use of rankings of research organisations in research assessment
Commitment: 5. Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
Commitment: 6. Review and develop research assessment criteria, tools and processes
Commitment: 7. Raise awareness of research assessment reform and provide transparent communication, guidance, and training on assessment criteria and processes as well as their use
Commitment: 8. Exchange practices and experiences to enable mutual learning within and beyond the Coalition
Commitment: 9. Communicate progress made on adherence to the Principles and implementation of the Commitments
Commitment: 10. Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research
Actor: Funders and regional authorities
Actor: Research institutions
Actor: Academies and learned societies
Actor: Research groups
Actor: Meta-researchers
-->

::: {.callout-warning icon=false}

## Objectives and potential outcome
<!--
PROMPTS:
- What happens if this is successful? What changes if this works? 
- What improvements are we aiming for?
- These can be related to the main questions
- If there are multiple experiments fitting, put them here
-->

- Empower the researcher and research institutes to take ownership of their own assessment 
- Foster transparency of the evaluation process
- Add potential benefits to the review process (e.g. building collaborative links between evaluators and applicants)
- Diversifying the reviewer pool along equity and subject matter grounds 
- Broaden who is considered an expert reviewer beyond traditional 

:::

## Research domains
<!--
PROMPTS:
- Which domain(s) can this experiment be applied in? 
-->

All domains, not specific for any domain


## Context and considerations
<!--
PROMPTS:
- Who is carrying out the evaluation? Who is this experiment for? (i.e., Target groups - Funders? Institutions? Individual researchers? Research domains?); 
- What type of assessment does this work on (Team/individual/project/institutional);
- What is the objective of evaluation exercise linked to the type of assessment
- What are the things that change in the experiment?
- Nature of the organization carrying out evaluation exercise; What type of assessment (Team/individual/project/institutional); Objective of evaluation exercise linked to the type of assessment
-->

**Implementation suggestions:**
- Decide assessment process logistics before start (virtual vs. in person, timings, etc)
- Depending on scope of assessment, determine timing and intermediate steps (e.g. feedback to applicants, etc.)
- Determine if there are veto rights by certain stakeholders on who is chosen as an evaluator
- Doesn’t actually save resources for administration, efforts are shifted to different parts of the process than searching for evaluators
- Provide clear guidance and limitations in support of a diversified and inclusive peer reviewer pool
- Applicants must suggest more than the needed amount, to allow for rejections
- Determine how many of the suggested peer reviewers to add to the actual pool of evaluators
- ALTERNATIVE - provide names of who not to contact. 
- Consider people who are not researchers, but rather users of research or other relevant profiles - from other sectors of society
- Community of assessors should reflect the community of applicants
- Be open and transparent - make it clear to the community/applicants who is reviewing (depends on scope of evaluation)

**Experiments can consider a variety of elements, including:**
- Who picks the reviewers
- Level of assessment (individual, project, research group, institution)
- Criteria for picking the reviewers (e.g. geographic, gender, etc..)
- Consider people who are not researchers, but rather users of research or other relevant profiles
- Definition of “Peer” depends on community of applicants - e.g. early career researchers should be evaluating early career fellowships

## Challenges and mitigations
<!--
PROMPTS:
- Each challenge and mitigation grouped together
- If there isn’t mitigation, say so
- Tips and tricks for preparing and running the experiment
-->

**Possible challenges**
- Maximizing diversity in reviewer choice - not always picking the same people
- Potential Conflicts of Interest - need to be very sure
- Reviewers are sometimes confused at this process change, need onboarding
- Biases due to known/unknown relationships (personal, professional, reputational)

**Possible mitigations**
- Create clear guidelines for how to suggest reviewers (could increase diversity/reduce bias)
- Clear communication for the experiment for all stakeholders
- Need to double check anyways for conflict of interest - Keep the process, and reviewers should declare
- Clear instructions for the evaluators (e.g. bias awareness training)
- Legal coverage - make applicants claim that there is no CoI
- Consider who suggests the peer reviewer and their relationship/what they stand to gain
- make it clear to the community/applicants who is reviewing (depends on scope of evaluation)
- Adjusting weight of different reviewers, be clear on the weight in advance


## Evaluating success
<!--
PROMPTS:
- Evaluation criteria for the experiment 
- How do you know it worked or didn’t?
-->

Diverse elements can be considered to evaluate the success of the experiment, including:
- Diversity of the evaluator pool (geographic, gender, new vs old partners, etc.)
- Self-assessment feedback from applicants (including off-target effects - does this create a real benefit for the applicants?)
- More non-research experts involved in peer review
- Diversity of the awardees
- More specific knowledge in the field for peer review
- Reduced time investment by funders/evaluation admin to find peer reviewers
- Science communication outcomes - more will hear about it/have a stake in it
- Specific purpose-built (based on objective for the assessment) indicator of the quality of assessment


## Related CoARA commitments
<!--
PROMPTS:
Directly reference as many of the 10 CoARA commitments as are fitting
-->

# Relevant resources and literature 

This section includes resources, literature, and reports relevant to this specific experimental idea.

Self-selected reviewers are common practice in journal peer review. Although research assessment differs depending on the elements being assessed (individual, project, career, research output, scientific paper, etc.), reviewing the elements that were raised in the early discussions of self-selected peer-reviewers for paper submissions can provide useful insights that may be relevant to consider in other types of assessment. Short articles from the Retraction watch in this regard are available [here](https://retractionwatch.com/2016/12/12/journals-new-program-choose-reviewers-get-decision-days/) and [here](https://retractionwatch.com/2011/04/08/should-authors-be-encouraged-to-pick-their-own-peer-reviewers/). 

## Templates from funders and institutions 
<!--
PROMPTS:
- Add templates from funders and institutions if existing
-->

## Case examples and literature
<!--
PROMPTS:
Any organisation names, links, authors that may be relevant to look into
-->

## Other resources
<!--
PROMPTS:
Include any other resource here
-->


# Comments/lived examples
<!--
PROMPTS:
Section to be kept blank, for input within the idea catalogue
-->


