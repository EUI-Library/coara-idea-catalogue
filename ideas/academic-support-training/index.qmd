---
title: "Academic support training"
author: "Experiments in Assessment WG"
date: "2025-03-03"
categories: [Level 0, User - Institutes]
description: "Academic support training equips staff to guide researchers through assessment processes by providing information, training, and tools to monitor evaluation of their work, units, and careers. This fosters transparency, empowers researchers, and drives culture change towards fairer, more responsible research assessment practices."
image: false
---

<!--
GENERAL GUIDELINES:
- Use this guide to style all text: https://quarto.org/docs/authoring/markdown-basics.html
- Tags (apply as relevant):
Level 0 = Level of completeness: 0 – The experiment only contains the description and minimal details. This level is meant to provide inspiration for experiments that can be developed further by those experimenting.
Level 1 = Level of completeness: 1 –  The experiment contains minimal details. Organisations will need to build the experimentation plan before they can implement the idea.
Level 2 = Level of completeness: 2 – The experiment contains minimal details that may help with the design and implementation, but organisations will need to build most of the experimentation plan before they can fully implement the idea.
Level 3 = Level of completeness: 3 – The experiment contains some details that may help with the design and implementation, but organisations will need to finalise the experimentation plan before they can fully implement the idea.
Level 4 = Level of completeness: 4 – The experiment contains details that can provide useful guidance for implementation, but additional research will need to be done by experimenting organisation. Few real life cases exist. 
Level 5 = Level of completeness: 5 – The experiment contains sufficient details to be implemented, and it has already been experimented in some organisations providing cases, additional suggestions, and often learned lessons. 
Challenge - Process = Create an assessment process based upon what the different players in research value
Challenge - Inclusivity = Make research more inclusive and equitable through assessment
Challenge - Bias Mitigation = Mitigate the negative effects of bias on assessment
Challenge - Process Culture = Recognize process and culture through assessment
Challenge - Diversity = Foster diversity through assessment
Challenge - Collaboration = Foster collaboration through assessment
Challenge - Different Questions = Ask assessment questions differently
CoARA Commitment 1 = Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
CoARA Commitment 2 = Base research assessment primarily on qualitative evaluation for which peer review is central, supported by responsible use of quantitative indicators
CoARA Commitment 3 = Abandon inappropriate uses in research assessment of journal- and publication-based metrics, in particular inappropriate uses of Journal Impact Factor (JIF) and h-index
CoARA Commitment 4 = Avoid the use of rankings of research organisations in research assessment
CoARA Commitment 5 = Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
CoARA Commitment 6 = Review and develop research assessment criteria, tools and processes
CoARA Commitment 7 = Raise awareness of research assessment reform and provide transparent communication, guidance, and training on assessment criteria and processes as well as their use
CoARA Commitment 8 = Exchange practices and experiences to enable mutual learning within and beyond the Coalition
CoARA Commitment 9 = Communicate progress made on adherence to the Principles and implementation of the Commitments
CoARA Commitment 10 = Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research
User - Funder = User could involve Funders and regional authorities
User - Institutes = User could involve Research institutions
User - Academies = User could involve Academies and learned societies
User - Research Groups = User could involve Research groups
User - Meta-Researchers = User could involve Meta-researchers
User - Scientific editors and publishers = User could involve Scientific editors and publishers
-->

::: {.callout-warning icon=false}

## Objectives and potential outcome
<!--
PROMPTS:
- What happens if this is successful? What changes if this works? 
- What improvements are we aiming for?
- These can be related to the main questions
- If there are multiple experiments fitting, put them here
-->

If successful, academic support training will increase researchers’ understanding and engagement in assessment processes, enhancing transparency and trust. It will empower researchers to actively manage their evaluation, improve the fairness and accuracy of assessments, and promote a culture shift toward responsible, inclusive, and trajectory-based research evaluation.

:::

## Research domains
<!--
PROMPTS:
- Which domain(s) can this experiment be applied in? 
-->

This academic support training can be applied across all research domains, as the need to understand and navigate evaluation processes is universal. It is particularly valuable in interdisciplinary fields, where assessment criteria may vary, and for early-career researchers who require guidance to align their work with institutional and funder expectations.

## Context and considerations
<!--
PROMPTS:
- Who is carrying out the evaluation? Who is this experiment for? (i.e., Target groups - Funders? Institutions? Individual researchers? Research domains?); 
- What type of assessment does this work on (Team/individual/project/institutional);
- What is the objective of evaluation exercise linked to the type of assessment
- What are the things that change in the experiment?
- Nature of the organization carrying out evaluation exercise; What type of assessment (Team/individual/project/institutional); Objective of evaluation exercise linked to the type of assessment
-->

Nature of the Evaluating Organization:
- Academic institutions (universities, research centers) vs. funding agencies vs. governmental bodies.
- Organizational culture and readiness for change affect training adoption and impact.

Type of Evaluation:
- Individual researcher assessment (e.g., tenure, promotion).
- Team or project-level evaluation (collaborative grants, group performance).
- Institutional evaluation (university rankings, strategic reviews).

Objectives of the Evaluation Exercise:
- Career progression and talent development.
- Resource allocation and funding decisions.
- Compliance with open science, equity, or diversity mandates.
- Quality assurance and accountability.

Implications for Training Design:
- Content must align with evaluation type and organizational goals.
- Delivery methods may vary (online modules, workshops, mentoring).
- Tools for transparency and self-monitoring should be customized accordingly.


## Challenges and mitigations
<!--
PROMPTS:
- Each challenge and mitigation grouped together
- If there isn’t mitigation, say so
- Tips and tricks for preparing and running the experiment
-->

**Challenge: Resistance to culture change among staff and researchers**
**Mitigation:** 
- Implement phased, trajectory-based training to gradually build acceptance.
- Use champions or early adopters to promote benefits and share success stories.
- Provide incentives and recognize participation.


**Challenge: Diverse baseline knowledge and engagement levels across staff roles (HR, researchers, administrators)**
**Mitigation:**
- Customize training content by role and career stage.
- Use modular and flexible formats (self-paced online, live sessions). 
If no mitigation: Assess user needs prior to design.


**Challenge: Lack of time and resources for staff to participate fully**
**Mitigation:**
- Integrate training into existing professional development frameworks.
- Provide microlearning options to reduce time burden.
If no mitigation: Seek management support for protected training time.

**Challenge: Measuring effectiveness and impact of culture change interventions**
**Mitigation:**
- Define clear KPIs and collect pre/post-training feedback.
- Use longitudinal tracking of behavior and assessment outcomes.
If no mitigation: Use qualitative feedback and case studies as alternative metrics.

**Challenge: Ensuring sustained engagement throughout the research lifecycle**
**Mitigation:**
- Design ongoing support and refresher training sessions.
- Implement mentoring and peer support networks.


## Evaluating success
<!--
PROMPTS:
- Evaluation criteria for the experiment 
- How do you know it worked or didn’t?
-->

**Evaluation Criteria**

- **Training Uptake and Completion Rates**
  - Percentage of staff (researchers, HR, others) completing training modules.
  - Active participation in live sessions or webinars.


- **Knowledge Gain and Skill Development**
  - Pre- and post-training assessments to measure knowledge improvement on evaluation processes.
  - Self-assessments on acquired skills related to evaluation tools and policies.

- **Changes in Behavior and Practice**
  - Actual use of support tools provided (e.g., evaluation dashboards).
  - Changes in evaluation management practices by staff and researchers.

- **Perceived Support and Satisfaction**
  - Qualitative and quantitative feedback through satisfaction surveys and interviews.
  - Level of perceived support and usefulness of training.


- **Impact on Organizational Culture**
  - Indicators of cultural change (e.g., increased transparency, awareness, collaboration).
  - Adoption of policies and behaviors aligned with responsible evaluation culture.

- **Effects on Evaluation Outcomes**
  - Improvement in quality and accuracy of evaluations conducted.
  - Reduction in complaints or disputes related to evaluation processes.

**How to Recognize Success**
- Significant and sustained increase in training participation.
- Clear evidence of improved knowledge and evaluation practices.
- Positive feedback and testimonials from engaged and more aware users.
- Tangible changes in evaluation procedures and greater transparency.
- Reduction of issues in evaluation management.


## Related CoARA commitments
<!--
PROMPTS:
Directly reference as many of the 10 CoARA commitments as are fitting
-->

# Relevant resources and literature 

This section includes resources, literature, and reports relevant to this specific experimental idea.

**Initiatives: Vitae (UK)**
Offers comprehensive training, mentoring, and 'Train the Trainer' support across the Researcher Development Framework. They also design Development Needs Analysis to underpin programs and enhance evaluation to evidence impact. <https://vitae.ac.uk/policy/our-projects/>

**University of Oxford (UK)**
Program: "Leading in Academic Research Environments" – A pilot program co-designed by Oxford academics and leadership experts, aimed at fostering positive, effective, and collaborative research environments. [Website](https://researchsupport.admin.ox.ac.uk/leading-in-academic-research-environments)

**University of Stirling (UK)**
Recognition: Received the HR Excellence in Research Award, recognizing the university's commitment to supporting researchers' careers, improving research quality and impact, and encouraging internal culture change. [Website](https://www.stir.ac.uk/research/research-culture/)

**Sapienza University of Rome (Italy)**
Program: Organizes a cross-disciplinary training program on soft skills aimed at enhancing the training path of Early-Stage Researchers, focusing on future non-academic careers. [Website](https://www.uniroma1.it/en/pagina/sapienza-training-offer-soft-skills-early-stage-researchers)

**National Institute for Health and Care Research (NIHR, UK)**
Support: Provides schemes and courses to help researchers conduct research in health and care settings, including training in research leadership skills and career development opportunities. [Website](https://www.nihr.ac.uk/about-us/what-we-do/training-researchers)

**Politecnico di Milano (Italy) and Politecnico di Torino**
Initiative: Participates in the HR Excellence in Research (HREiR) Award, aligning its HR policies with the principles set out in the European Charter for Researchers to create a favorable academic environment. See links [here](https://www.polimi.it/en/research/how-we-do-research-at-politecnico/hr-excellence-in-research) and [here](https://www.polito.it/ateneo/lavora-e-collabora-con-noi/percorsi-e-carriere-di-ricerca/hr-excellence-in-research)


## Templates from funders and institutions 
<!--
PROMPTS:
- Add templates from funders and institutions if existing
-->

## Case examples and literature
<!--
PROMPTS:
Any organisation names, links, authors that may be relevant to look into
-->

## Other resources
<!--
PROMPTS:
Include any other resource here
-->


# Comments/lived examples
<!--
PROMPTS:
Section to be kept blank, for input within the idea catalogue
-->



