---
title: "Distributed Peer-Review"
author: "Experiments in Assessment WG"
date: "2025-04-24"
categories: [Level 4, Challenge - Inclusivity, Challenge - Process Culture, Challenge - Collaboration, CoARA Commitment 2, CoARA Commitment 6, CoARA Commitment 10, User - Funders, User - Institutes, User - Meta-Researchers]
description: "Distributed peer review is a method in which applicants evaluate each other’s proposals. It promotes equity, transparency, and inclusion, reduces the burden on traditional reviewers, and values the expertise of participants by making them active contributors to the assessment process."
image: false
---

<!--
GENERAL GUIDELINES:
- Use this guide to style all text: https://quarto.org/docs/authoring/markdown-basics.html
- Tags (apply as relevant):
Level 0 = Level of completeness: 0 – The experiment only contains the description and minimal details. This level is meant to provide inspiration for experiments that can be developed further by those experimenting.
Level 1 = Level of completeness: 1 –  The experiment contains minimal details. Organisations will need to build the experimentation plan before they can implement the idea.
Level 2 = Level of completeness: 2 – The experiment contains minimal details that may help with the design and implementation, but organisations will need to build most of the experimentation plan before they can fully implement the idea.
Level 3 = Level of completeness: 3 – The experiment contains some details that may help with the design and implementation, but organisations will need to finalise the experimentation plan before they can fully implement the idea.
Level 4 = Level of completeness: 4 – The experiment contains details that can provide useful guidance for implementation, but additional research will need to be done by experimenting organisation. Few real life cases exist. 
Level 5 = Level of completeness: 5 – The experiment contains sufficient details to be implemented, and it has already been experimented in some organisations providing cases, additional suggestions, and often learned lessons. 
Challenge - Process = Create an assessment process based upon what the different players in research value
Challenge - Inclusivity = Make research more inclusive and equitable through assessment
Challenge - Bias Mitigation = Mitigate the negative effects of bias on assessment
Challenge - Process Culture = Recognize process and culture through assessment
Challenge - Diversity = Foster diversity through assessment
Challenge - Collaboration = Foster collaboration through assessment
Challenge - Different Questions = Ask assessment questions differently
CoARA Commitment 1 = Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
CoARA Commitment 2 = Base research assessment primarily on qualitative evaluation for which peer review is central, supported by responsible use of quantitative indicators
CoARA Commitment 3 = Abandon inappropriate uses in research assessment of journal- and publication-based metrics, in particular inappropriate uses of Journal Impact Factor (JIF) and h-index
CoARA Commitment 4 = Avoid the use of rankings of research organisations in research assessment
CoARA Commitment 5 = Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
CoARA Commitment 6 = Review and develop research assessment criteria, tools and processes
CoARA Commitment 7 = Raise awareness of research assessment reform and provide transparent communication, guidance, and training on assessment criteria and processes as well as their use
CoARA Commitment 8 = Exchange practices and experiences to enable mutual learning within and beyond the Coalition
CoARA Commitment 9 = Communicate progress made on adherence to the Principles and implementation of the Commitments
CoARA Commitment 10 = Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research
User - Funder = User could involve Funders and regional authorities
User - Institutes = User could involve Research institutions
User - Academies = User could involve Academies and learned societies
User - Research Groups = User could involve Research groups
User - Meta-Researchers = User could involve Meta-researchers
User - Scientific editors and publishers = User could involve Scientific editors and publishers
-->

::: {.callout-warning icon=false}

## Objectives and potential outcome
<!--
PROMPTS:
- What happens if this is successful? What changes if this works? 
- What improvements are we aiming for?
- These can be related to the main questions
- If there are multiple experiments fitting, put them here
-->

This experiment addresses key challenges in research assessment by promoting inclusion and equity through broader participation, mitigating bias by diversifying the reviewer pool, and fostering collaboration as applicants engage directly in the peer review process, gaining insight into evaluation criteria and building mutual understanding within the research community.


**Goals (what are we trying to change)**

- **Improve inclusivity and transparency**
 - Make the review process more accessible and participatory, reducing reliance on closed senior reviewer networks.

- **Address reviewer scarcity**
 - Involving applicants as reviewers overcomes recruitment challenges and distributes workload more evenly.

- **Enhance fairness in interdisciplinary contexts**
 - In multi-domain calls, the diversity of the review pool increases, enabling broader, multidimensional evaluations.

- **Foster better understanding of the review process**
 - Applicants who review become more familiar with criteria, improving their proposal writing and self-assessment skills over time.

- **Reduce reviewer fatigue**
 - Sharing review responsibilities among all participants prevents overburdening a small group of traditional reviewers.


- **Ensure domain expertise**
 - In focused calls, the applicant pool is knowledgeable, providing relevant expertise for peer review.

- **Accelerate the review process**
 - Since all participants have a stake and understand the criteria, the process can be completed more rapidly.


How does an experiment with distributed peer review work? 
During an application round, applicants review each other’s proposals, giving feedback and recommendations for acceptance/rejection. Quality control based on peer-review is thus restricted to those who have submitted a proposal.

:::

## Research domains
<!--
PROMPTS:
- Which domain(s) can this experiment be applied in? 
-->

This experiment can be applied across all research domains, particularly in contexts where applicant expertise aligns with the call topic. It involves a distributed peer review model in which applicants evaluate each other’s proposals, ensuring that quality control remains within a knowledgeable and invested community of peers.

## Context and considerations
<!--
PROMPTS:
- Who is carrying out the evaluation? Who is this experiment for? (i.e., Target groups - Funders? Institutions? Individual researchers? Research domains?); 
- What type of assessment does this work on (Team/individual/project/institutional);
- What is the objective of evaluation exercise linked to the type of assessment
- What are the things that change in the experiment?
- Nature of the organization carrying out evaluation exercise; What type of assessment (Team/individual/project/institutional); Objective of evaluation exercise linked to the type of assessment
-->

This experiment can fit any target group where assessment includes several similar applications and where applicants have a minimal expertise of each other's domains. This includes:

**Funders**: looking for innovative, scalable evaluation models to reduce review fatigue and improve equity.
**Institutions**: especially those conducting internal or competitive funding calls, or managing doctoral/postdoctoral fellowships.
**Thematic consortia or networks**: where members share disciplinary language and goals, easing implementation.

Potential variables that may have an influence on the distributed peer review process and may thus be valuable elements to experiment on include the **identity of reviewers** (e.g., anonymity vs. known identities), the **size and diversity of the reviewer pool**, and the **language used in proposals and reviews**, which may affect accessibility, clarity, and fairness across different linguistic or cultural backgrounds.

**Suggestions on how to implement**

**Pilot in Small-Scale Calls**:
Start with **small funding schemes** or **thematic calls** (e.g. internal grants, early-career researcher awards), where the administrative complexity and risks are lower. This allows for iterative refinement.

**Focus on Single-Domain or Thematic Areas**:
Use **single-discipline** or **well-defined thematic areas** to ensure a more homogeneous applicant pool with shared evaluation norms and language, reducing ambiguity and potential bias.

**Run Shadow Experiments**:
Before full implementation, run **parallel (shadow) experiments** alongside traditional reviews. Compare results to evaluate feasibility, reliability, and stakeholder satisfaction.

**Integrate Peer Review Training**:
Provide **training modules** to help applicants understand the evaluation criteria and write constructive feedback. This is particularly useful for **Early Career Researchers**, improving both their evaluative skills and grant writing.


**Implementing distributed peer review requires careful planning and resourcing**:
* **HR and Coordination Costs**:
+ **Project Manager**: to coordinate implementation, timeline, logistics.
+ **Administrative Support**: for applicant communication, managing review assignments.
* **Legal and Ethical Considerations**:
+ **Legal Advisers**: to ensure GDPR compliance, particularly around anonymization and data handling.
+ Develop **terms of participation** and conflict of interest policies.
* **Technical Infrastructure**:
+ **Platform or Tool**: for proposal submission, anonymized review distribution, and scoring.
+ Ensure **User Experience (UX)** is intuitive and accessible.
* **Training and Support**:
+ Develop or license **training materials** for applicants as reviewers.
+ Consider **helpdesk support** during the review period.
* **Communication and Engagement**:
+ Clearly explain the purpose, process, and benefits to participants.
+ Highlight how reviews are used and how quality is monitored to foster trust.


## Challenges and mitigations
<!--
PROMPTS:
- Each challenge and mitigation grouped together
- If there isn’t mitigation, say so
- Tips and tricks for preparing and running the experiment
-->

1. Potential reviewer bias
 Challenge: Reviewers may consciously or unconsciously favor or penalize proposals.
 Mitigation: Implement a double-blind review process where both reviewer and applicant identities are anonymized. Provide bias-awareness training as part of the review onboarding.

2. Conflicts of interest in small domains
 Challenge: In niche fields, applicants may know each other, creating risks of favoritism or retaliation.
 Mitigation: Require conflict of interest declarations, and use automated tools to help detect potential overlaps. Enable applicants to flag conflicts during the assignment process.

3. Transparency of competition
 Challenge: Ensuring that the review process remains fair and trustworthy.
 Mitigation: Provide clear evaluation criteria, publish aggregated review statistics (e.g., number of reviews per proposal, average scores), and offer post-review debriefings when possible.

4. Fostering motivation for high-quality reviews
 Challenge: Applicants may see the task as a burden or strategic opportunity, leading to superficial reviews.
 Mitigation: Introduce incentives, such as recognition, certificates, or bonus points for quality reviews. Consider meta-reviews or calibration scores to assess and reward reviewer effort and reliability.

5. Quality control of the peer review
 Challenge: Ensuring consistency and fairness across multiple reviewers.
 Mitigation: Introduce a review moderation system or involve meta-reviewers who assess the quality of peer reviews. Use rubrics and training materials to standardize expectations.


## Evaluating success
<!--
PROMPTS:
- Evaluation criteria for the experiment 
- How do you know it worked or didn’t?
-->

**How to assess effects/outcomes – Data to collect**
* **Results of Shadow Experiments**
+ Compare outcomes of distributed peer review with traditional review within the same call.
+ Analyze **agreement rates** between the two methods (e.g., overlap in selected proposals).

* **Review Quality Metrics**
+ Use meta-evaluation forms to assess reviews for accuracy, thoroughness, and constructiveness.
+ Involve expert reviewers or independent panels for spot quality checks.

* **User Feedback and Surveys**
+ Collect post-call surveys on:
    - Experience as both reviewers and applicants
    - Perceptions of **fairness and transparency**
    - Perceived **workload**
    - Understanding of evaluation criteria

* **Evaluation Timelines**
+ Track overall process speed: average time from submission to notification.
+ Compare with timelines of similar calls evaluated by traditional methods.


## Related CoARA commitments
<!--
PROMPTS:
Directly reference as many of the 10 CoARA commitments as are fitting
-->

# Relevant resources and literature 
  
(RoRI), Research on Research Institute; Stafford, Tom; Pinfield, Stephen; Butters, Anna; Benson Marshall, Melanie (2025). RoRI Insights: Applicants as reviewers - a Guide to Distributed Peer Review. Research on Research Institute. Report. [https://doi.org/10.6084/m9.figshare.29270534.v1](https://doi.org/10.6084/m9.figshare.29270534.v1) 
Pearson. How to speed up peer review: make applicants mark one another. Nature. 2025 Jul;643(8071):313-314. doi: 10.1038/d41586-025-02090-z
[https://assets.publishing.service.gov.uk/media/685a83af72588f418862071d/a-year-in-metascience-2025.pdf](https://assets.publishing.service.gov.uk/media/685a83af72588f418862071d/a-year-in-metascience-2025.pdf)


## Templates from funders and institutions 
<!--
PROMPTS:
- Add templates from funders and institutions if existing
-->

## Case examples and literature
<!--
PROMPTS:
Any organisation names, links, authors that may be relevant to look into
-->

The  Volkswagen Foundation has been experimenting with distributed peer review for some time. More information can be found at: [https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method](https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method) 

The Humboldt Foundation has been experimenting with a linked concept, namely Review Circle. In Review Circles, instead of two separate written reviews per application, a group of six to ten reviewers compare and discuss several applications on a protected digital platform. More information can be found at: [https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment](https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment) 

The Alexander von Humboldt Foundation in Germany is currently experimenting with combined peer review formats, and has already had positive experiences. Specifically, instead of requesting peer reviews for proposals separately, the review could be done combinedly on a digital, interactive platform where researchers exchange their reviews/comments and get to discuss about the quality and innovativeness of proposals. This way, the reviewers could also directly compare the proposals with each other and rank them. [https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment](https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment) 

Iterative assessments, where proposals are submitted and may be accepted, rejected or provided with feedback for the investigators to respond to, before being re-considered for funding are also being investigated with through the indigenous funding space at the Canadian Institutes of Health Research. There was a similar process formerly used in the Randomized Controlled Trials Committees of their old Open Operating Grants Program called UCR (under Continuing Review), where if the committee had simple questions that could make or break a proposal, they could rate the application provisionally based on satisfactory response to the questions. If the application then fell within the funding cut-off, the applicant would receive the question(s) and have 5 business days to respond. if the response was satisfactory they would get funded, if it wasn't they would be deemed unfundable and would need to re-apply in a future competition. 

The University of Antwerp is working on Comparative judgment (e.g. D-PAC), a concept that could be relevant when experimenting with distributive peer review. 

More experiments about distributed peer review, aplicants for funding review each other's proposals can be found from the Research on Research Institute (RoRI) at: 
[https://researchonresearch.org/volkswagen-distributed-peer-review/](https://researchonresearch.org/volkswagen-distributed-peer-review/) 


## Other resources
<!--
PROMPTS:
Include any other resource here
-->

# Comments/lived examples
<!--
PROMPTS:
Section to be kept blank, for input within the idea catalogue
-->

[https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method](https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method) 
Astronomy institutions (e.g. European Southern Observatory) use distributed peer review for awarding telescope observation time

[https://doi.eso.org/10.18727/0722-6691/5147](https://doi.eso.org/10.18727/0722-6691/5147) 

