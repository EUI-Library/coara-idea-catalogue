<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Experiments in Assessment WG">
<meta name="dcterms.date" content="2025-04-24">
<meta name="description" content="Distributed peer review is a method in which applicants evaluate each other’s proposals. It promotes equity, transparency, and inclusion, reduces the burden on traditional reviewers, and values the expertise of participants by making them active contributors to the assessment process.">

<title>Distributed Peer-Review – Assessment Idea Catalogue</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-121309c02025b111abb877319683f178.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/logo-CoARA-menu-white.svg" alt="" class="navbar-logo light-content">
    <img src="../../images/logo-CoARA-menu-white.svg" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Assessment Idea Catalogue</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../how_to.html"> 
<span class="menu-text">How-to Guide</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About the Idea Catalogue</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/EUI-Library/coara-idea-catalogue"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Distributed Peer-Review</h1>
                  <div>
        <div class="description">
          Distributed peer review is a method in which applicants evaluate each other’s proposals. It promotes equity, transparency, and inclusion, reduces the burden on traditional reviewers, and values the expertise of participants by making them active contributors to the assessment process.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Level 4</div>
                <div class="quarto-category">Challenge - Inclusivity</div>
                <div class="quarto-category">Challenge - Process Culture</div>
                <div class="quarto-category">Challenge - Collaboration</div>
                <div class="quarto-category">CoARA Commitment 2</div>
                <div class="quarto-category">CoARA Commitment 6</div>
                <div class="quarto-category">CoARA Commitment 10</div>
                <div class="quarto-category">User - Funders</div>
                <div class="quarto-category">User - Institutes</div>
                <div class="quarto-category">User - Meta-Researchers</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Experiments in Assessment WG </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 24, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<!--
GENERAL GUIDELINES:
- Use this guide to style all text: https://quarto.org/docs/authoring/markdown-basics.html
- Tags (apply as relevant):
Level 0 = Level of completeness: 0 – The experiment only contains the description and minimal details. This level is meant to provide inspiration for experiments that can be developed further by those experimenting.
Level 1 = Level of completeness: 1 –  The experiment contains minimal details. Organisations will need to build the experimentation plan before they can implement the idea.
Level 2 = Level of completeness: 2 – The experiment contains minimal details that may help with the design and implementation, but organisations will need to build most of the experimentation plan before they can fully implement the idea.
Level 3 = Level of completeness: 3 – The experiment contains some details that may help with the design and implementation, but organisations will need to finalise the experimentation plan before they can fully implement the idea.
Level 4 = Level of completeness: 4 – The experiment contains details that can provide useful guidance for implementation, but additional research will need to be done by experimenting organisation. Few real life cases exist. 
Level 5 = Level of completeness: 5 – The experiment contains sufficient details to be implemented, and it has already been experimented in some organisations providing cases, additional suggestions, and often learned lessons. 
Challenge - Process = Create an assessment process based upon what the different players in research value
Challenge - Inclusivity = Make research more inclusive and equitable through assessment
Challenge - Bias Mitigation = Mitigate the negative effects of bias on assessment
Challenge - Process Culture = Recognize process and culture through assessment
Challenge - Diversity = Foster diversity through assessment
Challenge - Collaboration = Foster collaboration through assessment
Challenge - Different Questions = Ask assessment questions differently
CoARA Commitment 1 = Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
CoARA Commitment 2 = Base research assessment primarily on qualitative evaluation for which peer review is central, supported by responsible use of quantitative indicators
CoARA Commitment 3 = Abandon inappropriate uses in research assessment of journal- and publication-based metrics, in particular inappropriate uses of Journal Impact Factor (JIF) and h-index
CoARA Commitment 4 = Avoid the use of rankings of research organisations in research assessment
CoARA Commitment 5 = Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
CoARA Commitment 6 = Review and develop research assessment criteria, tools and processes
CoARA Commitment 7 = Raise awareness of research assessment reform and provide transparent communication, guidance, and training on assessment criteria and processes as well as their use
CoARA Commitment 8 = Exchange practices and experiences to enable mutual learning within and beyond the Coalition
CoARA Commitment 9 = Communicate progress made on adherence to the Principles and implementation of the Commitments
CoARA Commitment 10 = Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research
User - Funder = User could involve Funders and regional authorities
User - Institutes = User could involve Research institutions
User - Academies = User could involve Academies and learned societies
User - Research Groups = User could involve Research groups
User - Meta-Researchers = User could involve Meta-researchers
User - Scientific editors and publishers = User could involve Scientific editors and publishers
-->
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Objectives and potential outcome
</div>
</div>
<div class="callout-body-container callout-body">
<!--
PROMPTS:
- What happens if this is successful? What changes if this works? 
- What improvements are we aiming for?
- These can be related to the main questions
- If there are multiple experiments fitting, put them here
-->
<p>This experiment addresses key challenges in research assessment by promoting inclusion and equity through broader participation, mitigating bias by diversifying the reviewer pool, and fostering collaboration as applicants engage directly in the peer review process, gaining insight into evaluation criteria and building mutual understanding within the research community.</p>
<p><strong>Goals (what are we trying to change)</strong></p>
<ul>
<li><p><strong>Improve inclusivity and transparency</strong> Make the review process more accessible and participatory, reducing reliance on closed senior reviewer networks.</p></li>
<li><p><strong>Address reviewer scarcity</strong> Involving applicants as reviewers overcomes recruitment challenges and distributes workload more evenly.</p></li>
<li><p><strong>Enhance fairness in interdisciplinary contexts</strong> In multi-domain calls, the diversity of the review pool increases, enabling broader, multidimensional evaluations.</p></li>
<li><p><strong>Foster better understanding of the review process</strong> Applicants who review become more familiar with criteria, improving their proposal writing and self-assessment skills over time.</p></li>
<li><p><strong>Reduce reviewer fatigue</strong> Sharing review responsibilities among all participants prevents overburdening a small group of traditional reviewers.</p></li>
<li><p><strong>Ensure domain expertise</strong> In focused calls, the applicant pool is knowledgeable, providing relevant expertise for peer review.</p></li>
<li><p><strong>Accelerate the review process</strong> Since all participants have a stake and understand the criteria, the process can be completed more rapidly.</p></li>
</ul>
<p>How does an experiment with distributed peer review work? During an application round, applicants review each other’s proposals, giving feedback and recommendations for acceptance/rejection. Quality control based on peer-review is thus restricted to those who have submitted a proposal.</p>
</div>
</div>
<section id="research-domains" class="level2">
<h2 class="anchored" data-anchor-id="research-domains">Research domains</h2>
<!--
PROMPTS:
- Which domain(s) can this experiment be applied in? 
-->
<p>This experiment can be applied across all research domains, particularly in contexts where applicant expertise aligns with the call topic. It involves a distributed peer review model in which applicants evaluate each other’s proposals, ensuring that quality control remains within a knowledgeable and invested community of peers.</p>
</section>
<section id="context-and-considerations" class="level2">
<h2 class="anchored" data-anchor-id="context-and-considerations">Context and considerations</h2>
<!--
PROMPTS:
- Who is carrying out the evaluation? Who is this experiment for? (i.e., Target groups - Funders? Institutions? Individual researchers? Research domains?); 
- What type of assessment does this work on (Team/individual/project/institutional);
- What is the objective of evaluation exercise linked to the type of assessment
- What are the things that change in the experiment?
- Nature of the organization carrying out evaluation exercise; What type of assessment (Team/individual/project/institutional); Objective of evaluation exercise linked to the type of assessment
-->
<p>This experiment can fit any target group where assessment includes several similar applications and where applicants have a minimal expertise of each other’s domains. This includes:</p>
<p><strong>Funders</strong>: looking for innovative, scalable evaluation models to reduce review fatigue and improve equity. <strong>Institutions</strong>: especially those conducting internal or competitive funding calls, or managing doctoral/postdoctoral fellowships. <strong>Thematic consortia or networks</strong>: where members share disciplinary language and goals, easing implementation.</p>
<p>Potential variables that may have an influence on the distributed peer review process and may thus be valuable elements to experiment on include the <strong>identity of reviewers</strong> (e.g., anonymity vs.&nbsp;known identities), the <strong>size and diversity of the reviewer pool</strong>, and the <strong>language used in proposals and reviews</strong>, which may affect accessibility, clarity, and fairness across different linguistic or cultural backgrounds.</p>
<p><strong>Suggestions on how to implement</strong></p>
<p><strong>Pilot in Small-Scale Calls</strong>:<br>
Start with <strong>small funding schemes</strong> or <strong>thematic calls</strong> (e.g.&nbsp;internal grants, early-career researcher awards), where the administrative complexity and risks are lower. This allows for iterative refinement.</p>
<p><strong>Focus on Single-Domain or Thematic Areas</strong>:<br>
Use <strong>single-discipline</strong> or <strong>well-defined thematic areas</strong> to ensure a more homogeneous applicant pool with shared evaluation norms and language, reducing ambiguity and potential bias.</p>
<p><strong>Run Shadow Experiments</strong>:<br>
Before full implementation, run <strong>parallel (shadow) experiments</strong> alongside traditional reviews. Compare results to evaluate feasibility, reliability, and stakeholder satisfaction.</p>
<p><strong>Integrate Peer Review Training</strong>:<br>
Provide <strong>training modules</strong> to help applicants understand the evaluation criteria and write constructive feedback. This is particularly useful for <strong>Early Career Researchers</strong>, improving both their evaluative skills and grant writing.</p>
<p><strong>Implementing distributed peer review requires careful planning and resourcing</strong>:</p>
<ul>
<li><strong>HR and Coordination Costs</strong>:
<ul>
<li><em>Project Manager</em>: to coordinate implementation, timeline, logistics.<br>
</li>
<li><em>Administrative Support</em>: for applicant communication, managing review assignments.</li>
</ul></li>
<li><em>Legal and Ethical Considerations</em>:
<ul>
<li><em>Legal Advisers</em>: to ensure GDPR compliance, particularly around anonymization and data handling.<br>
</li>
<li>Develop <strong>terms of participation</strong> and conflict of interest policies.</li>
</ul></li>
<li><em>Technical Infrastructure</em>:
<ul>
<li><em>Platform or Tool</em>: for proposal submission, anonymized review distribution, and scoring.<br>
</li>
<li>Ensure <em>User Experience (UX)</em> is intuitive and accessible.</li>
</ul></li>
<li><em>Training and Support</em>:
<ul>
<li>Develop or license <em>training materials</em> for applicants as reviewers.<br>
</li>
<li>Consider <em>helpdesk support</em> during the review period.</li>
</ul></li>
<li><em>Communication and Engagement</em>:
<ul>
<li>Clearly explain the purpose, process, and benefits to participants.<br>
</li>
<li>Highlight how reviews are used and how quality is monitored to foster trust.</li>
</ul></li>
</ul>
</section>
<section id="challenges-and-mitigations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-mitigations">Challenges and mitigations</h2>
<!--
PROMPTS:
- Each challenge and mitigation grouped together
- If there isn’t mitigation, say so
- Tips and tricks for preparing and running the experiment
-->
<p><strong>1. Potential reviewer bias</strong><br>
<strong>Challenge:</strong> Reviewers may consciously or unconsciously favor or penalize proposals.<br>
<strong>Mitigation:</strong> Implement a double-blind review process where both reviewer and applicant identities are anonymized. Provide bias-awareness training as part of the review onboarding.</p>
<p><strong>2. Conflicts of interest in small domains</strong><br>
<strong>Challenge:</strong> In niche fields, applicants may know each other, creating risks of favoritism or retaliation.<br>
<strong>Mitigation:</strong> Require conflict of interest declarations, and use automated tools to help detect potential overlaps. Enable applicants to flag conflicts during the assignment process.</p>
<p><strong>3. Transparency of competition</strong><br>
<strong>Challenge:</strong> Ensuring that the review process remains fair and trustworthy.<br>
<strong>Mitigation:</strong> Provide clear evaluation criteria, publish aggregated review statistics (e.g., number of reviews per proposal, average scores), and offer post-review debriefings when possible.</p>
<p><strong>4. Fostering motivation for high-quality reviews</strong><br>
<strong>Challenge:</strong> Applicants may see the task as a burden or strategic opportunity, leading to superficial reviews.<br>
<strong>Mitigation:</strong> Introduce incentives, such as recognition, certificates, or bonus points for quality reviews. Consider meta-reviews or calibration scores to assess and reward reviewer effort and reliability.</p>
<p><strong>5. Quality control of the peer review</strong><br>
<strong>Challenge:</strong>Ensuring consistency and fairness across multiple reviewers.<br>
<strong>Mitigation:</strong> Introduce a review moderation system or involve meta-reviewers who assess the quality of peer reviews. Use rubrics and training materials to standardize expectations.</p>
</section>
<section id="evaluating-success" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-success">Evaluating success</h2>
<!--
PROMPTS:
- Evaluation criteria for the experiment 
- How do you know it worked or didn’t?
-->
<p><strong>How to assess effects/outcomes – Data to collect</strong> * <strong>Results of Shadow Experiments</strong><br>
+ Compare outcomes of distributed peer review with traditional review within the same call.<br>
+ Analyze <strong>agreement rates</strong> between the two methods (e.g., overlap in selected proposals).</p>
<ul>
<li><strong>Review Quality Metrics</strong>
<ul>
<li>Use meta-evaluation forms to assess reviews for accuracy, thoroughness, and constructiveness.<br>
</li>
<li>Involve expert reviewers or independent panels for spot quality checks.</li>
</ul></li>
<li><strong>User Feedback and Surveys</strong>
<ul>
<li>Collect post-call surveys on:
<ul>
<li>Experience as both reviewers and applicants<br>
</li>
<li>Perceptions of <strong>fairness and transparency</strong><br>
</li>
<li>Perceived <strong>workload</strong><br>
</li>
<li>Understanding of evaluation criteria</li>
</ul></li>
</ul></li>
<li><strong>Evaluation Timelines</strong>
<ul>
<li>Track overall process speed: average time from submission to notification.<br>
</li>
<li>Compare with timelines of similar calls evaluated by traditional methods.</li>
</ul></li>
</ul>
</section>
<section id="related-coara-commitments" class="level2">
<h2 class="anchored" data-anchor-id="related-coara-commitments">Related CoARA commitments</h2>
<!--
PROMPTS:
Directly reference as many of the 10 CoARA commitments as are fitting
-->
</section>
<section id="relevant-resources-and-literature" class="level1">
<h1>Relevant resources and literature</h1>
<p>(RoRI), Research on Research Institute; Stafford, Tom; Pinfield, Stephen; Butters, Anna; Benson Marshall, Melanie (2025). RoRI Insights: Applicants as reviewers - a Guide to Distributed Peer Review. Research on Research Institute. Report. <a href="https://doi.org/10.6084/m9.figshare.29270534.v1">https://doi.org/10.6084/m9.figshare.29270534.v1</a><br>
Pearson. How to speed up peer review: make applicants mark one another. Nature. 2025 Jul;643(8071):313-314. doi: 10.1038/d41586-025-02090-z<br>
<a href="https://assets.publishing.service.gov.uk/media/685a83af72588f418862071d/a-year-in-metascience-2025.pdf">https://assets.publishing.service.gov.uk/media/685a83af72588f418862071d/a-year-in-metascience-2025.pdf</a></p>
<section id="templates-from-funders-and-institutions" class="level2">
<h2 class="anchored" data-anchor-id="templates-from-funders-and-institutions">Templates from funders and institutions</h2>
<!--
PROMPTS:
- Add templates from funders and institutions if existing
-->
</section>
<section id="case-examples-and-literature" class="level2">
<h2 class="anchored" data-anchor-id="case-examples-and-literature">Case examples and literature</h2>
<!--
PROMPTS:
Any organisation names, links, authors that may be relevant to look into
-->
<p>The Volkswagen Foundation has been experimenting with distributed peer review for some time. More information can be found at: <a href="https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method">https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method</a></p>
<p>The Humboldt Foundation has been experimenting with a linked concept, namely Review Circle. In Review Circles, instead of two separate written reviews per application, a group of six to ten reviewers compare and discuss several applications on a protected digital platform. More information can be found at: <a href="https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment">https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment</a></p>
<p>The Alexander von Humboldt Foundation in Germany is currently experimenting with combined peer review formats, and has already had positive experiences. Specifically, instead of requesting peer reviews for proposals separately, the review could be done combinedly on a digital, interactive platform where researchers exchange their reviews/comments and get to discuss about the quality and innovativeness of proposals. This way, the reviewers could also directly compare the proposals with each other and rank them. <a href="https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment">https://www.humboldt-foundation.de/en/explore/figures-and-statistics/evaluation/evaluation-of-the-2022-peer-circle-experiment</a></p>
<p>Iterative assessments, where proposals are submitted and may be accepted, rejected or provided with feedback for the investigators to respond to, before being re-considered for funding are also being investigated with through the indigenous funding space at the Canadian Institutes of Health Research. There was a similar process formerly used in the Randomized Controlled Trials Committees of their old Open Operating Grants Program called UCR (under Continuing Review), where if the committee had simple questions that could make or break a proposal, they could rate the application provisionally based on satisfactory response to the questions. If the application then fell within the funding cut-off, the applicant would receive the question(s) and have 5 business days to respond. if the response was satisfactory they would get funded, if it wasn’t they would be deemed unfundable and would need to re-apply in a future competition.</p>
<p>The University of Antwerp is working on Comparative judgment (e.g.&nbsp;D-PAC), a concept that could be relevant when experimenting with distributive peer review.</p>
<p>More experiments about distributed peer review, aplicants for funding review each other’s proposals can be found from the Research on Research Institute (RoRI) at: <a href="https://researchonresearch.org/volkswagen-distributed-peer-review/">https://researchonresearch.org/volkswagen-distributed-peer-review/</a></p>
</section>
<section id="other-resources" class="level2">
<h2 class="anchored" data-anchor-id="other-resources">Other resources</h2>
<!--
PROMPTS:
Include any other resource here
-->
</section>
</section>
<section id="commentslived-examples" class="level1">
<h1>Comments/lived examples</h1>
<!--
PROMPTS:
Section to be kept blank, for input within the idea catalogue
-->
<p><a href="https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method">https://www.volkswagenstiftung.de/en/news/interview/volkswagen-foundation-experiments-new-peer-review-method</a> Astronomy institutions (e.g.&nbsp;European Southern Observatory) use distributed peer review for awarding telescope observation time</p>
<p><a href="https://doi.eso.org/10.18727/0722-6691/5147">https://doi.eso.org/10.18727/0722-6691/5147</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/EUI-Library\.github\.io\/coara-idea-catalogue\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Unless otherwise specified, content is licenced as <a href="https://creativecommons.org/licenses/by/4.0/"><img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg" class="img-fluid" alt="CC BY 4.0"></a></p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>