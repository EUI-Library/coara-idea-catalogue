---
title: "Hiring as a site of experimentation"
author: "Experiments in Assessment WG"
date: "2025-03-03"
categories: [Level 1, Challenge - Process, Challenge - Inclusivity, Challenge - Process Culture, Challenge - Diversity, Challenge - Different Questions, CoARA Commitment 1, CoARA Commitment 5, CoARA Commitment 6, CoARA Commitment 8, CoARA Commitment 10, User - Institutes, User - Meta-Researchers]
description: "Using the institutional hiring process to diversify staff profiles and expertise in line with the goals and needs of the institution."
image: false
---

<!--
GENERAL GUIDELINES:
- Use this guide to style all text: https://quarto.org/docs/authoring/markdown-basics.html
- Tags (apply as relevant):
Level 0 = Level of completeness: 0 – The experiment only contains the description and minimal details. This level is meant to provide inspiration for experiments that can be developed further by those experimenting.
Level 1 = Level of completeness: 1 –  The experiment contains minimal details. Organisations will need to build the experimentation plan before they can implement the idea.
Level 2 = Level of completeness: 2 – The experiment contains minimal details that may help with the design and implementation, but organisations will need to build most of the experimentation plan before they can fully implement the idea.
Level 3 = Level of completeness: 3 – The experiment contains some details that may help with the design and implementation, but organisations will need to finalise the experimentation plan before they can fully implement the idea.
Level 4 = Level of completeness: 4 – The experiment contains details that can provide useful guidance for implementation, but additional research will need to be done by experimenting organisation. Few real life cases exist. 
Level 5 = Level of completeness: 5 – The experiment contains sufficient details to be implemented, and it has already been experimented in some organisations providing cases, additional suggestions, and often learned lessons. 
Challenge - Process = Create an assessment process based upon what the different players in research value
Challenge - Inclusivity = Make research more inclusive and equitable through assessment
Challenge - Bias Mitigation = Mitigate the negative effects of bias on assessment
Challenge - Process Culture = Recognize process and culture through assessment
Challenge - Diversity = Foster diversity through assessment
Challenge - Collaboration = Foster collaboration through assessment
Challenge - Different Questions = Ask assessment questions differently
CoARA Commitment 1 = Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
CoARA Commitment 2 = Base research assessment primarily on qualitative evaluation for which peer review is central, supported by responsible use of quantitative indicators
CoARA Commitment 3 = Abandon inappropriate uses in research assessment of journal- and publication-based metrics, in particular inappropriate uses of Journal Impact Factor (JIF) and h-index
CoARA Commitment 4 = Avoid the use of rankings of research organisations in research assessment
CoARA Commitment 5 = Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
CoARA Commitment 6 = Review and develop research assessment criteria, tools and processes
CoARA Commitment 7 = Raise awareness of research assessment reform and provide transparent communication, guidance, and training on assessment criteria and processes as well as their use
CoARA Commitment 8 = Exchange practices and experiences to enable mutual learning within and beyond the Coalition
CoARA Commitment 9 = Communicate progress made on adherence to the Principles and implementation of the Commitments
CoARA Commitment 10 = Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research
User - Funder = User could involve Funders and regional authorities
User - Institutes = User could involve Research institutions
User - Academies = User could involve Academies and learned societies
User - Research Groups = User could involve Research groups
User - Meta-Researchers = User could involve Meta-researchers
User - Scientific editors and publishers = User could involve Scientific editors and publishers
-->

::: {.callout-warning icon=false}

## Objectives and potential outcome
<!--
PROMPTS:
- What happens if this is successful? What changes if this works? 
- What improvements are we aiming for?
- These can be related to the main questions
- If there are multiple experiments fitting, put them here
-->

- Brings more diversity and plurality in the academic ecosystem and career models
- Promotes more collaboration between academic and other sectors
- Makes Universities/institutions “actors of change” through developing people as “actors of change”
- Makes the academic sector more approachable/attractive for everyone (esp. People that don’t know it)


:::

## Research domains
<!--
PROMPTS:
- Which domain(s) can this experiment be applied in? 
-->


## Context and considerations
<!--
PROMPTS:
- Who is carrying out the evaluation? Who is this experiment for? (i.e., Target groups - Funders? Institutions? Individual researchers? Research domains?); 
- What type of assessment does this work on (Team/individual/project/institutional);
- What is the objective of evaluation exercise linked to the type of assessment
- What are the things that change in the experiment?
- Nature of the organization carrying out evaluation exercise; What type of assessment (Team/individual/project/institutional); Objective of evaluation exercise linked to the type of assessment
-->

This idea is especially relevant for **institutional hiring**, and **career development for existing staff/researcher**. Experimentation in this regard is more likely to take place in individual assessments run by assessment committees.  

Preparation and guidance is key, as well as a clear/transparent process. This is crucial given the implication of hiring decisions on individuals' lives. 

Regulations must be adhered to, but also potentially modified to fit. Yet, fairness needs to remain a crucial element for consideration in any processes, and experimental changes should probably be avoided within a single call to ensure consistent evaluation of candidates within a call.  
  
**Hiring as a site for experimentation could be used in many different ways. For example, this could take the form of:**  

* Developing people more broadly, or mainly evaluating people for a more broad profile  
* Tandem professorship (exists in Germany) - How to ensure that people aren’t only covering the typical aspects of research (and education to a lesser extent)   
* Bringing people with different career paths into academia (e.g. industry, public sector, practitioners)  
* Can also be potentially two people with different expertises collaborating  
* Intersectoral mobility between academia and other domains
* Look at the overall strategy/needs of the unit rather than at individual level - don’t think individually, more systematically
  + First step: teams/groups need to learn how to think about their development as a group - “what do we need in the system/unit?”
* Expanding career models  
  + Start with Junior professorship (academic path) - acquire experience in the next years - either academic or joint between academic/industry - building a more broad set of skills
* Rethinking the hiring requirements  
  + e.g., Teaching and research are equally important - but education is “forgotten” in hiring - Narrative CV developed to abolish distinction between teaching/research - all professors (even those focusing on research) must teach as part of hiring procedure  


## Challenges and mitigations
<!--
PROMPTS:
- Each challenge and mitigation grouped together
- If there isn’t mitigation, say so
- Tips and tricks for preparing and running the experiment
-->

Entry into an academic career - person hired stays a really long time!
Career development requirements and incentives - mandatory staying up-to-date
Careful with quality assurance and career model fitting


* **Challenge:** Regulations around are quite strict, hard to experiment
* **Mitigation:**
  + Easier to evaluate “what everyone knows” - people want to be careful  

* **Challenge:** Hiring committee needs to be targeted for guidance/process change
* **Mitigation:**
  + Building good guidance and process for assessment should be standard practice even without experimentation. 

* **Challenge:** Hiring committee is very diverse - rules about who is represented are strict
* **Mitigation:** 
  + The composition can be modified - e.g. person focusing on EDI, etc.. 
  + More training for hiring committees - e.g. assessment reform  

* **How to get more diverse people in the hiring committees?**
* **Mitigation:**
  + Depends on institutional regulations - these can potentially be modified. 
  + Need to have enough diverse people in the institution to make it work (look at the issues getting gender diversity on committees with few women having to do everything)  
  
* **Challenge:** How to reach more diverse people and make them apply?
* **Mitigation:** 
  + Using network with other sectors (e.g. industry) - people are usually interested in working part time, or trying an academic career again
  + Fostering intersectoral mobility

* **Challenge:** Important to be fair and transparent - Lots of rules here
* **Mitigation:**
  + Experimental changes should probably be avoided within a single call to ensure consistent evaluation of candidates within a call

* **Additional challenges:**
  + There’s a fallback to just “replace what was lost” when professors leave
  + Shouldn’t hire diverse people just for that sake, it should be towards a bigger strategic target
  + Competitive to get a professorship - Lots of really good people who don’t fit the more diversity target - it’s tough enough to be a good researcher
  + System level view - always need people who cover the research part to ensure the experimental elements and conclusions are sound and evidence-based


## Evaluating success
<!--
PROMPTS:
- Evaluation criteria for the experiment 
- How do you know it worked or didn’t?
-->

While success will depend on the objective for the experimentation, certain questions may help situate whether the experimental assessment is successful or not.

* Which areas are important to highlight/develop for the institution (e.g. research, teaching, and practice - maybe others) and has the experimental assessment pursued these areas?
* Are the diverse dimensions assessed and developed in individuals or in the breadth of profiles existing in an institution?
* Do individuals (and their skills) meet the needs coverage for the unit?
  + Profile plan/resources plan for teams/institutes/departments/
  + Reflective evaluation on the plan (after a few years) and how the people in the unit are covering everything, and if the goals have been reached, and how the people contributed to this 
* Do the experimental assessment promote intersectoral mobility and collaboration? (i.e., if promotion of diversity of skills and sector is valued for the assessment)
* Are diversity distributions improved quantitatively?
  + (e.g. how many people came from other institutions/sectors, what areas/focuses do they cover, experience level mix). 
  + Note that it is diffcule to have “what is good”/benchmarking - if you see a few areas with high coverage and others with low, then you see the weak spots.


## Related CoARA commitments
<!--
PROMPTS:
Directly reference as many of the 10 CoARA commitments as are fitting
-->

**CoARA Commitment 1** – Recognise the diversity of contributions to, and careers in, research in accordance with the needs and nature of the research
**CoARA Commitment 5** – Commit resources to reforming research assessment as is needed to achieve the organisational changes committed to
**CoARA Commitment 6** – Review and develop research assessment criteria, tools and processes
**CoARA Commitment 8** – Exchange practices and experiences to enable mutual learning within and beyond the Coalition
**CoARA Commitment 10** – Evaluate practices, criteria and tools based on solid evidence and the state-of-the-art in research on research, and make data openly available for evidence gathering and research

# Relevant resources and literature 

This section includes resources, literature, and reports relevant to this specific experimental idea (where available).

## Templates from funders and institutions 
<!--
PROMPTS:
- Add templates from funders and institutions if existing
-->

## Case examples and literature
<!--
PROMPTS:
Any organisation names, links, authors that may be relevant to look into
-->

## Other resources
<!--
PROMPTS:
Include any other resource here
-->


# Comments/lived examples
<!--
PROMPTS:
Section to be kept blank, for input within the idea catalogue
-->





